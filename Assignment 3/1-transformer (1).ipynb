{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: DIY Transformer and Language Modeling\n",
    "\n",
    "In this part of the assignment we will build our own Transformer architecture and see how it can be used for causal (\"left to right\" or \"decoder-only\") language modeling, similar to the GPT line of models. We will train a very simple language model for demonstration purposes, examining how we can train a model in a self-supervised fashion and then generate text autoregressively. \n",
    "\n",
    "The model we train in this part will not be impressive in and of itself. Pretraining an effective language model requires days, weeks, or months of compute time on several GPUs over large quantities of data, which would be impractical for a learning assignment (not to mention the environmental impact). Nevertheless, this part of the assignment will help us to develop a deeper understanding of the transformer architecture that undergirds essentially all language models. In later parts, we will perform inference and fine-tuning with much larger and more performant large language models that have already been pretrained. \n",
    "\n",
    "**Learning objectives.** You will:\n",
    "1. Implement the transformer architecture in PyTorch\n",
    "2. Pretrain a casual small language model in a self-supervised manner\n",
    "3. Use a causual language model to autoregressively generate text\n",
    "\n",
    "While it is possible to complete this assignment using CPU compute, it may be slow. To accelerate your training, consider using GPU resources such as `CUDA` through the CS department cluster. Alternatives include Google colab or local GPU resources for those running on machines with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "This transformer implementation will be DIY in the sense that you may not use the `Transformer` module in PyTorch, but you will otherwise use PyTorch extensively. In particular, it is expected that you will use the following modules, substantially simplifying the implementation.\n",
    "- [`Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) for both the word embedding and the positional encoding/embedding.\n",
    "- [`MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)\n",
    "- [`LayerNorm`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n",
    "- [`Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "- [`ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) or other nonlinear activations of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a standard transformer block:\n",
    "    1. multi-head self-attention (with custom MultiHeadAttention)\n",
    "    2. residual add and layer norm\n",
    "    3. position-wise feedforward MLP with a single hidden layer\n",
    "    4. another residual add and layer norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_embed, num_heads):\n",
    "        super().__init__()\n",
    "        # Custom multi-head attention layer\n",
    "        self.attn = MultiHeadAttention(d_embed, num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_embed)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_embed, 256),  # Feedforward hidden layer size (256)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, d_embed)\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(d_embed)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        \"\"\"\n",
    "        Unbatched input: x should be a rank 2 tensor with \n",
    "        a row per input token and column per embedding dimension.\n",
    "        attn_mask should have same shape as x and be passed to\n",
    "        the MultiHeadAttention modules forward method.\n",
    "        Output should have the same shape as x.\n",
    "        \"\"\"\n",
    "        # Multi-head Self Attention + Residual connection\n",
    "        attn_output = self.attn(x, x, x, mask=attn_mask)\n",
    "        x = self.layer_norm1(x + attn_output)  # Residual connection and layer norm\n",
    "\n",
    "        # Feedforward Network + Residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layer_norm2(x + ffn_output)  # Residual connection and layer norm\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a standard decoder-only transformer \n",
    "    for causal language modeling: \n",
    "    1. input word embedding from vocab_size to d_embed\n",
    "    2. add positional embedding (support max_length tokens)\n",
    "    3. pass through n_blocks TransformerBlocks\n",
    "    4. unembedding linear layer to vocab_size\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_embed=64, num_heads=4, max_length=64, n_blocks=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
    "        self.positional_embedding = nn.Embedding(max_length, d_embed)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_embed, num_heads) for _ in range(n_blocks)])\n",
    "\n",
    "        self.fc_out = nn.Linear(d_embed, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Unbatched input: x should be a rank 1 tensor with \n",
    "        indices of the tokens within the vocabulary. \n",
    "        Output should be a rank 2 tensor with a row per input token\n",
    "        containing unnormalized logits over the vocabulary.\n",
    "        Note: Must compute causal attn_mask to provide to any\n",
    "        Transformer blocks.\n",
    "        \"\"\"\n",
    "        seq_length = x.size(0) \n",
    "        device = x.device \n",
    "        \n",
    "        token_embeds = self.embedding(x) \n",
    "        positions = torch.arange(0, seq_length, device=device) \n",
    "        positional_embeds = self.positional_embedding(positions)  \n",
    "        x = token_embeds + positional_embeds \n",
    "        \n",
    "        attn_mask = torch.triu(torch.ones(seq_length, seq_length, device=device), diagonal=1) == 0\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask)\n",
    "\n",
    "        logits = self.fc_out(x)  # (seq_length, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we wrote our more extensive pseudocode, then provided that pseudocode based on the lecture slides for each section to ChatGPT for code implementations before writing and implementing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Now we will train our transformer as a causal language model on the following `simple_sentences` dataset consisting of 96 single-sentence statements about cats. We are intentionally using this very small *toy* dataset so that you can efficiently train and experiment with your model. Clearly it is an inadequate amount of text for generating a large complex language model, which is generally very computationally expensive.\n",
    "\n",
    "Don't forget to run the following cell to define `simple_sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run, but you do not need to modify this code\n",
    "\n",
    "simple_sentences = [\"Cats are furry animals that many people keep as pets.\",\n",
    "    \"Most cats have soft fur that comes in many colors and patterns.\",\n",
    "    \"Cats have sharp claws that they use for climbing and scratching.\",\n",
    "    \"Many cats enjoy sleeping for long hours during the day.\",\n",
    "    \"Cats are known for their ability to always land on their feet when they fall.\",\n",
    "    \"Kittens are baby cats that are very playful and cute.\",\n",
    "    \"Cats have excellent night vision, which helps them hunt in the dark.\",\n",
    "    \"Most cats are very good at keeping themselves clean by licking their fur.\",\n",
    "    \"Cats communicate with each other and with humans by meowing.\",\n",
    "    \"Many cats enjoy chasing and playing with small toys.\",\n",
    "    \"Cats have a strong sense of balance and can walk on narrow surfaces easily.\",\n",
    "    \"Some cats like to sit on windowsills and watch the world outside.\",\n",
    "    \"Cats have rough tongues that feel like sandpaper when they lick you.\",\n",
    "    \"Many cats enjoy being petted and will purr to show they are happy.\",\n",
    "    \"Cats are often independent animals that don't need constant attention.\",\n",
    "    \"Some cats like to knock things off tables and shelves for fun.\",\n",
    "    \"Cats have whiskers that help them sense their surroundings.\",\n",
    "    \"Many cats are good at catching mice and other small animals.\",\n",
    "    \"Some cats enjoy playing with laser pointers, chasing the red dot.\",\n",
    "    \"Cats often stretch after waking up from a nap.\",\n",
    "    \"Many cats like to sit in boxes, even if the box seems too small for them.\",\n",
    "    \"Cats have retractable claws that they can extend when needed.\",\n",
    "    \"Some cats enjoy playing with water, while others avoid it.\",\n",
    "    \"Cats have a keen sense of smell that helps them find food.\",\n",
    "    \"Many cats like to perch on high places to observe their surroundings.\",\n",
    "    \"Cats often knead their paws on soft surfaces, which is called 'making biscuits'.\",\n",
    "    \"Some cats are very vocal and will meow a lot to get attention.\",\n",
    "    \"Cats have excellent hearing and can detect very quiet sounds.\",\n",
    "    \"Many cats enjoy chasing strings or ribbons as a form of play.\",\n",
    "    \"Cats often groom each other as a sign of affection.\",\n",
    "    \"Some cats like to hide in small spaces when they feel scared or stressed.\",\n",
    "    \"Cats have a third eyelid called the nictitating membrane that protects their eyes.\",\n",
    "    \"Many cats enjoy basking in warm sunlight coming through windows.\",\n",
    "    \"Cats use their tails for balance when walking on narrow surfaces.\",\n",
    "    \"Some cats are very social and enjoy the company of humans and other cats.\",\n",
    "    \"Cats have scent glands on their cheeks that they use to mark their territory.\",\n",
    "    \"Many cats enjoy climbing trees and exploring high places.\",\n",
    "    \"Cats often bring their owners 'gifts' like toys or small animals they've caught.\",\n",
    "    \"Some cats are more active at night, reflecting their natural hunting instincts.\",\n",
    "    \"Cats have flexible bodies that allow them to squeeze through small spaces.\",\n",
    "    \"Many cats enjoy playing with catnip, which can make them very excited.\",\n",
    "    \"Cats use their whiskers to measure whether they can fit through openings.\",\n",
    "    \"Some cats like to sleep on their backs with their paws in the air.\",\n",
    "    \"Cats have a strong sense of territory and may not like other cats in their space.\",\n",
    "    \"Many cats enjoy scratching posts to keep their claws healthy and mark territory.\",\n",
    "    \"Cats often show affection by rubbing their heads against people or objects.\",\n",
    "    \"Some cats are very curious and will investigate new objects in their environment.\",\n",
    "    \"Cats have a special reflective layer in their eyes that helps them see in low light.\",\n",
    "    \"Many cats enjoy chasing and pouncing on moving objects.\",\n",
    "    \"Cats use their tails to communicate their mood and intentions.\",\n",
    "    \"Some cats like to sit on warm surfaces like laptops or freshly dried laundry.\",\n",
    "    \"Cats have excellent balance and can often walk along fences or railings.\",\n",
    "    \"Many cats are good jumpers and can leap several times their own height.\",\n",
    "    \"Cats often knead their paws when they're feeling comfortable and content.\",\n",
    "    \"Some cats like to drink running water from faucets or fountains.\",\n",
    "    \"Cats have a keen sense of hearing and can rotate their ears to locate sounds.\",\n",
    "    \"Many cats enjoy playing with interactive toys that move or make noise.\",\n",
    "    \"Cats often groom themselves after eating to clean their faces and paws.\",\n",
    "    \"Some cats are very food-motivated and will do tricks for treats.\",\n",
    "    \"Cats have a strong hunting instinct, even if they're well-fed house pets.\",\n",
    "    \"Many cats enjoy sitting on laps and cuddling with their owners.\",\n",
    "    \"Cats use their tails for balance when running and making quick turns.\",\n",
    "    \"Some cats like to 'talk' to birds they see through windows.\",\n",
    "    \"Cats have a unique gait where both legs on one side move together.\",\n",
    "    \"Many cats enjoy playing with paper bags or cardboard boxes.\",\n",
    "    \"Cats often hide when they're not feeling well or are in pain.\",\n",
    "    \"Some cats like to sleep in unusual positions that look uncomfortable to humans.\",\n",
    "    \"Cats have a good memory and can remember people and places for a long time.\",\n",
    "    \"Many cats enjoy being brushed, which helps keep their coat healthy.\",\n",
    "    \"Cats use their sense of smell to identify other cats and people.\",\n",
    "    \"Some cats are very playful and will initiate games with their owners.\",\n",
    "    \"Cats have a natural instinct to cover their waste in litter or soil.\",\n",
    "    \"Many cats enjoy watching fish in aquariums or birds outside.\",\n",
    "    \"Cats often show affection by slow blinking, which is like a kitty kiss.\",\n",
    "    \"Some cats like to follow their owners from room to room.\",\n",
    "    \"Cats have a good sense of time and often know when it's mealtime.\",\n",
    "    \"Many cats enjoy sitting in sunny spots to warm themselves.\",\n",
    "    \"Cats use their tails to help them balance when walking on narrow surfaces.\",\n",
    "    \"Some cats are very vocal and have a wide range of meows and other sounds.\",\n",
    "    \"Cats have excellent reflexes and can quickly dodge obstacles.\",\n",
    "    \"Many cats enjoy playing with crinkly toys or balls with bells inside.\",\n",
    "    \"Cats often knead their paws on soft surfaces as a sign of contentment.\",\n",
    "    \"Some cats like to sleep on their owner's bed or pillow.\",\n",
    "    \"Cats have a third eyelid that helps protect their eyes while hunting.\",\n",
    "    \"Many cats enjoy climbing cat trees or scratching posts.\",\n",
    "    \"Cats use their whiskers to help them navigate in the dark.\",\n",
    "    \"Some cats are very gentle and patient with children.\",\n",
    "    \"Cats have a strong sense of smell that helps them locate food and mates.\",\n",
    "    \"Many cats enjoy playing with interactive toys that challenge their hunting skills.\",\n",
    "    \"Cats often groom themselves to regulate their body temperature.\",\n",
    "    \"Some cats like to sit in high places to survey their surroundings.\",\n",
    "    \"Cats have a natural instinct to chase small, moving objects.\",\n",
    "    \"Many cats enjoy being petted under their chin or behind their ears.\",\n",
    "    \"Cats use their tails to communicate their mood and intentions to other cats.\",\n",
    "    \"Some cats are very affectionate and will seek out human companionship.\",\n",
    "    \"Cats have a good sense of balance and can often land on their feet when falling.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a **tokenizer** for the dataset. Later in this assignment we will use more sophisticated tokenizers that were prepared for more complex pretrained transformer models. \n",
    "\n",
    "Here, we define a very simple tokenizer that follows a similar API with an `encode` method that takes text as input and returns a list of indices corresponding to the positions of the tokens in the voacbulary, as well as a `decode` method that takes the list of token indices and returns the resulting space-separated string.\n",
    "\n",
    "Rather than a vocabulary fixed in advance, this tokenizer takes a list of sentences as input and determines a vocabulary as shown in the `__init__` constructor, which takes `sentences` as input when the tokenizer is created.\n",
    "1. The ending `.` is removed and the strings are moved to lowercase,\n",
    "2. The vocabulary is initialized to the unique space-separated words appearing in all sentences,\n",
    "3. Special tokens are added:\n",
    "    - `<unk>` Is a placeholder for anything not represented in the vocabulary\n",
    "    - `<sos>` Is a placeholder for the start of a sentence before any tokens\n",
    "    - `<eos>` Is a placeholder for the end of a sentence after any tokens\n",
    "\n",
    "The code defines the `Tokenizer`, initializes a `Tokenizer` object on the `simple_sentences` from above, and then demonstrates the use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 382\n",
      "First 5 tokens:  ['human', 'kitty', 'whiskers', 'affectionate', 'quick']\n",
      "Last 5 tokens:  ['measure', 'coming', '<unk>', '<sos>', '<eos>']\n",
      "[168, 346, 73, 379]\n",
      "cats are cute <unk>\n"
     ]
    }
   ],
   "source": [
    "# Run, but you do not need to modify this code\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, sentences):\n",
    "        sentences = [sentence.lower().strip(\".\") for sentence in sentences]\n",
    "        self.vocab = list(set(word for sentence in sentences for word in sentence.split()))\n",
    "        self.special_tokens = ['<unk>', '<sos>', '<eos>']\n",
    "        self.vocab += self.special_tokens\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.vocab)}\n",
    "        self.index_to_word = {index: word for word, index in self.word_to_index.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = text.lower().strip(\".\")\n",
    "        return [self.word_to_index.get(word, self.word_to_index['<unk>']) for word in text.split()]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ' '.join(self.index_to_word.get(idx, '<unk>') for idx in indices)\n",
    "\n",
    "# Example creating tokenizer\n",
    "tokenizer = Tokenizer(simple_sentences)\n",
    "print(\"Vocabulary size:\", len(tokenizer.vocab))\n",
    "print(\"First 5 tokens: \", tokenizer.vocab[:5])\n",
    "print(\"Last 5 tokens: \", tokenizer.vocab[-5:])\n",
    "\n",
    "# Example encoding\n",
    "encoded = tokenizer.encode(\"cats are cute fosho\")\n",
    "print(encoded)\n",
    "\n",
    "# Example decoding\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task here is to complete the implementation of the `SimpleLanguageDataset` that we will use for training our transformer model on the above data. It is a `PyTorch` `Dataset` as you have used before. We will not use a `DataLoader` as our model does not handle batched input. \n",
    "\n",
    "The methods you need to fill in are:\n",
    "1. `__init__` is the usual constructor, which takes the sentences (as a list of strings) and a tokenizer object as input.\n",
    "2. `__len__` is the method called if you later run `len(dataset)` for a `dataset` object of the `SimpleLanguageDataset` class. It should return the number of sentences/training examples.\n",
    "3. `__getitem__(self, idx)` is the method called if you later run `dataset[idx]` for a `dataset` object of the `SimpleLanguageDataset` class. It should return a tuple of two tensors, the first representing the tokenized input and the second representing the tokenized target, which is the same as the input but offset by 1. You can add the tokenized `<sos>` to the start of the input sequence and the tokenized `<eos>` to the start of the target sequence.\n",
    "4. `shuffle` is simply a mutator (does not need to return anything) that should randomly permute/shuffle the order of the dataset. This will be useful to randomly shuffle the data after each epoch of minibatch stochastic gradient descent during training, since we aren't using a `DataLoader` for our model that does not handle batched input. You are welcome to use the imported `shuffle` method from the `random` module, [documented here](https://docs.python.org/3/library/random.html#random.shuffle).\n",
    "\n",
    "When you are finished, the code creates a `SimpleLanguageDataset` and prints the input and target sequences of the first item. If everything is implemented correctly, you should see that both are rank 1 tensors with a sequence of integers, having the same values but offset by 1 and with the `<sos>` index at the start of the input sequence and the `<eos>` at the end of the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input sequence:  tensor([380, 168, 346, 150, 369,  98, 118, 193, 361,  30, 300])\n",
      "First target sequence:  tensor([168, 346, 150, 369,  98, 118, 193, 361,  30, 300, 381])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from random import shuffle\n",
    "\n",
    "class SimpleLanguageDataset(Dataset):\n",
    "    \"\"\" \n",
    "    PyTorch dataset for a list of sentences along with a tokenizer object.\n",
    "    To be used for training a causal language model. Does not batch the\n",
    "    input: Treats each sentence in sentences as a single example and \n",
    "    __getitem__ returns a tokenized input and offset target sequence\n",
    "    in two separate tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences, tokenizer):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with a list of sentences and a tokenizer object.\n",
    "        Stores the tokenized sentences for use in training.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_sentences = [self.tokenizer.encode(sentence) for sentence in sentences]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of sentences/training examples\"\"\"\n",
    "        return len(self.tokenized_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the tokenized input and target sequence corresponding\n",
    "        to the sentence at index idx. The input sequence has a <sos> token at \n",
    "        the start, and the target sequence is offset by 1 with an <eos> token \n",
    "        at the end.\n",
    "        \n",
    "        Returns:\n",
    "            tuple of two tensors: (input_tensor, target_tensor)\n",
    "        \"\"\"\n",
    "        # Tokenized sentence\n",
    "        tokenized_sentence = self.tokenized_sentences[idx]\n",
    "        \n",
    "        # Create input by adding <sos> token at the beginning\n",
    "        input_sequence = [self.tokenizer.word_to_index['<sos>']] + tokenized_sentence\n",
    "        oon\n",
    "        # Create target by adding <eos> token at the end and offsetting by 1\n",
    "        target_sequence = tokenized_sentence + [self.tokenizer.word_to_index['<eos>']]\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        input_tensor = torch.tensor(input_sequence, dtype=torch.long)\n",
    "        target_tensor = torch.tensor(target_sequence, dtype=torch.long)\n",
    "        \n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"Randomly shuffles the sentences for use in SGD per epoch.\"\"\"\n",
    "        shuffle(self.tokenized_sentences)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(simple_sentences)\n",
    "dataset = SimpleLanguageDataset(simple_sentences, tokenizer)\n",
    "\n",
    "print(\"First input sequence: \",  dataset[0][0])\n",
    "print(\"First target sequence: \", dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we needed to use GPT to add the sos token to the beginning of each sentence, and convert from list to tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Now we actually want to train our transformer model as a causal language model on the dataset defined above using PyTorch. Your goal in this task is to train to achieve a training accuracy of at least 70% (noting that accuracy of 100% is neither possible nor desirable, since for example the phrase `\"Many cats\"` has many different next words in the training data).\n",
    "1. Though the model only works on a single sequence at a time (that is, does not handle batches of sequences), note that for every token in the input sequence a prediction will be made for the next token. A couple implications: (i) You can compute the average loss across the entire target sequence for use in training, and (ii) there are multiple classifications per input sequence for consideration in the training accuracy.\n",
    "2. Your model should use  `num_heads` and `n_layers` at least 2 and at most 16. You should choose the embedding `dimension` to be at least 32 and at most 512. Keep in mind that `dimension` should be divisible by `num_heads`, which is one reason why you see powers of 2 are common choices. Increasing these values increases the model capacity but may make training slower and more difficult.\n",
    "3. You are welcome to use the [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) or [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer, whichever you prefer. As always, you may need to experiment to find a good learning rate or to decide on other optimization hyperparameters like momentum.\n",
    "4. You should track and evaluate the average training loss (cross entropy) and the accuracy (next token classification), printing this information at least every epoch of training. We are focusing purely on modeling the training data and not on statistical generalization, so you do not need to evaluate a separate validation score or use any regularization techniques such as dropout.\n",
    "\n",
    "Once you are finished, briefly explain your model architecture and describe the total number of model parameters (you are welcome to calculate this by hand or in code as you prefer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 3.0325, Accuracy: 16.67%\n",
      "Epoch 2/20, Loss: 1.3329, Accuracy: 41.67%\n",
      "Epoch 3/20, Loss: 0.8159, Accuracy: 75.00%\n",
      "Epoch 4/20, Loss: 0.6568, Accuracy: 66.67%\n",
      "Epoch 5/20, Loss: 0.6119, Accuracy: 66.67%\n",
      "Epoch 6/20, Loss: 0.6078, Accuracy: 75.00%\n",
      "Epoch 7/20, Loss: 0.5413, Accuracy: 75.00%\n",
      "Epoch 8/20, Loss: 0.5165, Accuracy: 75.00%\n",
      "Epoch 9/20, Loss: 0.5280, Accuracy: 75.00%\n",
      "Epoch 10/20, Loss: 0.5128, Accuracy: 75.00%\n",
      "Epoch 11/20, Loss: 0.5092, Accuracy: 75.00%\n",
      "Epoch 12/20, Loss: 0.5642, Accuracy: 66.67%\n",
      "Epoch 13/20, Loss: 0.5259, Accuracy: 75.00%\n",
      "Epoch 14/20, Loss: 0.5342, Accuracy: 75.00%\n",
      "Epoch 15/20, Loss: 0.5327, Accuracy: 75.00%\n",
      "Epoch 16/20, Loss: 0.5315, Accuracy: 75.00%\n",
      "Epoch 17/20, Loss: 0.5161, Accuracy: 75.00%\n",
      "Epoch 18/20, Loss: 0.4987, Accuracy: 75.00%\n",
      "Epoch 19/20, Loss: 0.5209, Accuracy: 75.00%\n",
      "Epoch 20/20, Loss: 0.5036, Accuracy: 75.00%\n",
      "Total number of parameters: 862479\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CausalLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, num_heads=4, num_layers=4):\n",
    "        super(CausalLanguageModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 512, embedding_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=embedding_dim * 4, dropout=0.1\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :seq_len, :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataset, tokenizer, num_epochs=10, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for idx in range(len(dataset)):\n",
    "            input_sequence, target_sequence = dataset[idx]\n",
    "            input_sequence = input_sequence.unsqueeze(0)  \n",
    "            target_sequence = target_sequence.unsqueeze(0)  \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_sequence)\n",
    "            \n",
    "            logits = logits[:, :-1, :].reshape(-1, logits.size(-1)) \n",
    "            target_sequence = target_sequence[:, 1:].reshape(-1)     \n",
    "            \n",
    "            loss = criterion(logits, target_sequence)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_correct += (logits.argmax(dim=-1) == target_sequence).sum().item()\n",
    "            total_tokens += target_sequence.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        accuracy = total_correct / total_tokens * 100\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "\n",
    "simple_sentences = [\"Hello world\", \"How are you?\", \"Many cats sleep\", \"Some dogs bark loudly.\"]\n",
    "tokenizer = Tokenizer(simple_sentences)\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "dataset = SimpleLanguageDataset(simple_sentences, tokenizer)\n",
    "embedding_dim = 128 \n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "\n",
    "model = CausalLanguageModel(vocab_size, embedding_dim, num_heads, num_layers)\n",
    "\n",
    "train_model(model, dataset, tokenizer, num_epochs=20, learning_rate=0.001)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a CausalLanguageModel using a Transformer architecture, which first begins with an embedding layer that converts input tokens into vectors of size embedding_dim, contributing vocab_size * embedding_dim parameters. Next, a learnable positional encoding is added for positional information, contributing 512 * embedding_dim parameters. The main component is the Transformer encoder, consisting of num_layers layers, each with self-attention and feed-forward network layers. The self-attention mechanism has num_heads * embedding_dim * embedding_dim parameters, while the feed-forward network contributes embedding_dim * 4 * embedding_dim * 2 parameters per layer. The final linear output layer projects the transformer output to a vector of size vocab_size, contributing embedding_dim * vocab_size parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, you will autoregressively generate text using a simple random sampling with temperature. That is, given a starting input sequence, the model should generate up to `max_tokens` additional tokens. These are generated one at a time, at each step passing the entire input sequence (including any already generated tokens) as input to the model. Use the outputs of of the **last input token only**, which should be the unnormalized logits over the vocabulary. \n",
    "\n",
    "Divide these unnormalized logits by the `temperature` parameter then normalize these to a probability distribution over the vocabulary using [`softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) and then [sample a next token from this probability distribution](https://pytorch.org/docs/stable/generated/torch.multinomial.html).\n",
    "\n",
    "If the generated token is corresponds to `'<eos>'` then you should stop the process, or if you generate a total of `max_tokens` new tokens.\n",
    "\n",
    "For example, suppose we want to generate a completion of the `start_text = \"<sos> Many cats\"`. We would tokenize this and feed it as input to the model, and get the outputs corresponding to the last input token. We normalize these outputs as a probability distribution. Suppose we draw the index corresponding to `\"like\"` in the vocabulary from that probability distribution. Then to generate the next token, we pass the tokenized `\"<sos> Many cats like\"` as input to the model, repeating this process until we generate `\"<eos>\"` or reach `max_tokens` generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, start_text=\"<sos>\", max_tokens=20, temperature=1):\n",
    "    \"\"\"\n",
    "    Should return a string corresponding to up to max_tokens \n",
    "    autoregressively generated tokens beginning from start_text.\n",
    "    \"\"\"\n",
    "    # todo: implement generate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate your `generate` function below by selecting **at least 9** experiments, three each of three different types that vary as follows. You can use the default `max_tokens=20` everywhere.\n",
    "1. First, choose a simple prompt such as the default `\"<sos>\"` and vary the `temperature` parameter, trying values of `0.5`, the default of `1`, and a higher value of `2`.\n",
    "2. Next, leave the `temperature` to the default of `1` and try at least three prompts, each of which should have many different possible completions within the training data itself. For example, `\"<sos> Many cats\"`, `\"<sos> Some cats\"`, and `\"<sos> Cats have\"` all appear several times in the training data.\n",
    "3. Finally, again leaving the `temperature` to the default value of `1`, try at least three prompts each of which does not appear in the training dataset but should use words from the training dataset. For example `\"<sos> Cats like toys\"` is a correctly formed start to a sentence with words from the training data, but does not actually appear in the training data.\n",
    "\n",
    "For each example, print the `start_text` and the returned generated text. Briefly interpret your results. Specifically discuss the effect of the `temperature` parameter and the observed difference in results between type 2 and type 3 prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<sos>' | Temperature: 0.5\n",
      "Generated Text: '<sos> many some <unk> you? loudly <sos> <sos> sleep world sleep world some are <unk> are bark sleep many are bark'\n",
      "\n",
      "Prompt: '<sos>' | Temperature: 1\n",
      "Generated Text: '<sos> <sos> sleep you? sleep world <sos> <unk> sleep <unk> you? hello are sleep loudly how sleep many you? hello are'\n",
      "\n",
      "Prompt: '<sos>' | Temperature: 2\n",
      "Generated Text: '<sos> how bark are many <unk> some you? you? world are hello are <unk> cats bark <eos>'\n",
      "\n",
      "Prompt: '<sos> Many cats' | Temperature: 1\n",
      "Generated Text: '<sos> many cats you? hello are sleep dogs sleep are hello cats many you? <unk> many sleep dogs are how you? how you?'\n",
      "\n",
      "Prompt: '<sos> Some cats' | Temperature: 1\n",
      "Generated Text: '<sos> some cats some some you? bark sleep bark how are you? <eos>'\n",
      "\n",
      "Prompt: '<sos> Cats have' | Temperature: 1\n",
      "Generated Text: '<sos> cats <unk> <sos> bark how <unk> sleep world <eos>'\n",
      "\n",
      "Prompt: '<sos> Cats like toys' | Temperature: 1\n",
      "Generated Text: '<sos> cats <unk> <unk> world <sos> many how you? dogs world <unk> sleep world bark loudly hello bark sleep <unk> bark <eos>'\n",
      "\n",
      "Prompt: '<sos> Dogs like birds' | Temperature: 1\n",
      "Generated Text: '<sos> dogs <unk> <unk> dogs world <sos> are bark loudly dogs you? <sos> you? <eos>'\n",
      "\n",
      "Prompt: '<sos> Animals enjoy' | Temperature: 1\n",
      "Generated Text: '<sos> <unk> <unk> hello how <unk> hello some cats how how how you? <unk> <eos>'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(model, tokenizer, start_text=\"<sos>\", max_tokens=20, temperature=1):\n",
    "    \"\"\"\n",
    "    Generates text autoregressively up to max_tokens based on a start_text prompt.\n",
    "    \"\"\"\n",
    "    model.eval()  \n",
    "    generated_tokens = tokenizer.encode(start_text)\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        input_tensor = torch.tensor(generated_tokens).unsqueeze(0) \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)\n",
    "        \n",
    "        next_token_logits = logits[0, -1, :]  \n",
    "        \n",
    "        # Apply temperature and normalize to a probability distribution\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # Sample from the probability distribution\n",
    "        next_token = torch.multinomial(probabilities, 1).item()\n",
    "        \n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        # Stop if the generated token is <eos>\n",
    "        if next_token == tokenizer.word_to_index[\"<eos>\"]:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(generated_tokens)\n",
    "\n",
    "tokenizer = Tokenizer(simple_sentences)\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "embedding_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "model = CausalLanguageModel(vocab_size, embedding_dim, num_heads, num_layers)\n",
    "\n",
    "prompts = [\n",
    "    # Type 1\n",
    "    (\"<sos>\", 0.5),\n",
    "    (\"<sos>\", 1),\n",
    "    (\"<sos>\", 2),\n",
    "    \n",
    "    # Type 2\n",
    "    (\"<sos> Many cats\", 1),\n",
    "    (\"<sos> Some cats\", 1),\n",
    "    (\"<sos> Cats have\", 1),\n",
    "    \n",
    "    # Type 3\n",
    "    (\"<sos> Cats like toys\", 1),\n",
    "    (\"<sos> Dogs like birds\", 1),\n",
    "    (\"<sos> Animals enjoy\", 1)\n",
    "]\n",
    "\n",
    "for start_text, temp in prompts:\n",
    "    generated_text = generate(model, tokenizer, start_text=start_text, max_tokens=20, temperature=temp)\n",
    "    print(f\"Prompt: '{start_text}' | Temperature: {temp}\")\n",
    "    print(f\"Generated Text: '{generated_text}'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the observation of these results, one thing to note is the change of the temperature variable. \n",
    "\n",
    "In our type 1 sample test cases, a temperature of 0.5 generates repetitive, predictable text with minimal variation, often including unknown tokens (\"<unk>\"). The default temperature of 1 balances randomness and coherence, producing more varied but still understandable output. However, 2 results in more increased randomness, leading to less coherent and more erratic text.\n",
    "\n",
    "With Type 2 prompts of the training data, we can generate mostly relevant words, but often with nonsensical or incomplete sentences. Outputs are based on familiar patterns but still lack full coherence.\n",
    "    \n",
    "Finally, with type prompts, we struggle with novel combinations, generating incoherent text with frequent unknown tokens. The model relies on training data patterns but can't generalize well to unseen phrases. Lower temperatures produce more predictable output, while higher temperatures lead to more randomness. The model works best with familiar prompts but struggles with novel ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we prompted GPT to apply temperature and normalize a probability distribution, while also sampling from that same distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
