{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1001c433-4fca-4ba3-a670-10275b9274ad",
   "metadata": {},
   "source": [
    "# Part 3: Translation with Flan-T5\n",
    "\n",
    "In this part you will experiment with Google Flan-T5 models for machine translation. The original T5 model was a unifed sequence-to-sequence encoder-decoder architecture pretrained on a variety of tasks including machine translation. The Flan line of models improved on the performance of the original T5 series.\n",
    "\n",
    "In this part we will only apply the models, not train them. We will evaluate our results using the [Bleu score](https://en.wikipedia.org/wiki/BLEU), a common (but not perfect) quantitative metric for evaluating the quality of translations. Flan-T5 also comes in several different model sizes: We will study the impact of model size on performance by considering several different versions of Flan-T5.\n",
    "\n",
    "**Learning objectives.** You will:\n",
    "1. Examine an encoder-decoder sequence-to-sequence Flan-T5 transformer model\n",
    "2. Apply Flan-T5 models to perform machine translation\n",
    "3. Evaluate the quality of machine translations by computing Bleu scores with respect to reference translations\n",
    "4. Study the affect\n",
    "\n",
    "While it is possible to complete this assignment using CPU compute, it may be slow. To accelerate your training, consider using GPU resources such as `CUDA` through the CS department cluster. Alternatives include Google colab or local GPU resources for those running on machines with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5249219-8c67-4087-b2be-3f6c9982d951",
   "metadata": {},
   "source": [
    "First, ensure that you have the `transformers` and `datasets` modules installed. We will use these modules for importing tokenizers, pretrained models, and datasets. You can run the following cells to try to install them with `pip` if needed. If you are using ondemand, ideally you would simply include `module load transformers` and `module load datasets` when making your initial reservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bfb3fa3-39f4-45ca-989d-31019c339100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.11/site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.local/lib/python3.11/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.local/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f166e33-29ad-45b3-93f1-96b3f8173b62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.11/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.local/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.local/lib/python3.11/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in ./.local/lib/python3.11/site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.local/lib/python3.11/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9aba23-9b2c-4aec-beda-a00cefa676df",
   "metadata": {},
   "source": [
    "First we import the `flan-t5` tokenizer (shared across all model sizes) and demonstrate its characteristics and usage. Note that the API is the same as we saw previously for the `BERT` model -- may want to review the extra details in that earlier part.\n",
    "\n",
    "Note that the example contains both English and French text.\n",
    "\n",
    "(If you have trouble downloading the tokenizer, it is possible that you need to install the `sentencepiece` module, for example by `pip install sentencepiece`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9929b6b0-7992-4c34-8383-12f838676e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/fb/12/2f5c8d4764b00033cf1c935b702d3bb878d10be9f0b87f0253495832d85f/sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e54bab7-bdae-4c70-9d18-4c1708e479e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ipywidgets\n",
      "  Obtaining dependency information for ipywidgets from https://files.pythonhosted.org/packages/22/2d/9c0b76f2f9cc0ebede1b9371b6f317243028ed60b90705863d493bae622e/ipywidgets-8.1.5-py3-none-any.whl.metadata\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting comm>=0.1.3 (from ipywidgets)\n",
      "  Obtaining dependency information for comm>=0.1.3 from https://files.pythonhosted.org/packages/e6/75/49e5bfe642f71f272236b5b2d2691cf915a7283cc0ceda56357b61daa538/comm-0.2.2-py3-none-any.whl.metadata\n",
      "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from ipywidgets) (5.7.1)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Obtaining dependency information for widgetsnbextension~=4.0.12 from https://files.pythonhosted.org/packages/21/02/88b65cc394961a60c43c70517066b6b679738caf78506a5da7b88ffcb643/widgetsnbextension-4.0.13-py3-none-any.whl.metadata\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Obtaining dependency information for jupyterlab-widgets~=3.0.12 from https://files.pythonhosted.org/packages/a9/93/858e87edc634d628e5d752ba944c2833133a28fa87bb093e6832ced36a3e/jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: backcall in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, comm, ipywidgets\n",
      "Successfully installed comm-0.2.2 ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e637e38-7a1f-4eb3-91b4-e080daf23234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  32000\n",
      "{'input_ids': [[37, 385, 1001, 1712, 2085, 7, 16, 8, 2034, 1], [312, 4561, 3582, 9691, 5048, 247, 50, 25301, 1, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "print(\"Vocabulary size: \", tokenizer.vocab_size)\n",
    "tokenized = tokenizer([\"The little black cat sleeps in the window\", \n",
    "                       \"Le petit chat noir dort dans la fenêtre\"], padding='longest')\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b009142-7ad6-423d-9447-cf362ac3e6ae",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Below we import and preview the `flan-t5` model, beginning with the small version. You will also note that we are using 16-bit float representations to save on memory (this may be particularly relevant if you are using GPU compute for larger models, where the GPU may have limited memory available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef167a43-0dba-41eb-bb2f-7966b1a9b34c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 14:56:00.860131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# run, but you do not need to modify this code\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.float32)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe34963-7122-4021-bb4b-63b1d210c480",
   "metadata": {},
   "source": [
    "Examine the `model.parameters()`. How much memory (in kilobytes (KB), megabytes (MB), or gigabytes (GB)) should it take to store the model itself, given the 16-bit (or 2 byte) precision specified in the import? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41035ae7-9718-4d2e-acab-7d6d26117c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76961152\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b89f06-b05c-4882-b0fd-708fd2ed34a3",
   "metadata": {},
   "source": [
    "In the case of the 2 byte precision specified in the import, the memory in total bytes is the number of parameters * bytes per parameter, which is 76,961,152 * 2, approximately 154,000,000 bytes, or about 146.5 MB (dividing 154,000,000 by 1024 * 1024). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9a63d-7d7f-481f-a7a8-e7d87143c297",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Now we import and demonstrate the basic usage of the model `generate` method. This method autoregressively generates new text as we have discussed before in the context of causal language modeling, and supports different approaches (greedy, beam search, and sampling). The `generate` method API is [documented here](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/text_generation#transformers.GenerationMixin.generate).\n",
    "\n",
    "The example below demonstrates encoding a *batch* of inputs and passing them to the model for autoregressive generation. The printed output is generated by the model for the first and second input in the batch respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57177033-b02f-4e4a-afb0-f380313e3731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,   37, 1712,   19,    3,    9, 1712,    5,    1,    0],\n",
      "        [   0,   37, 1782,   19, 1180,   16,    8, 1057,    5,    1]])\n",
      "The cat is a cat.\n",
      "The dog is running in the field.\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "input_text = [\"The little black cat sleeps in the window\", \"The dog runs in the field\"]\n",
    "encoded = tokenizer(input_text, return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "outputs = model.generate(**encoded, max_new_tokens=100)\n",
    "print(outputs)\n",
    "\n",
    "for out in outputs:\n",
    "    print(tokenizer.decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa24589-2848-4928-b8c4-5869b9e31112",
   "metadata": {},
   "source": [
    "Note that the model did not translate the inputs. That is by design: Flan-T5 was pretrained on several different tasks including but not limited to machine translation.\n",
    "\n",
    "In order to use the model for translation, we need to **prompt it** to do so, providing context (literally) connecting to its pretraining. There are several different promptings that should work, we demonstrate two below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f8c4dc-11cd-401f-bb8a-c08fe7baba8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La petite cat noir s'est en vertu de la fenêtre.\n",
      "Le chien s'est en l'égard de la field.\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "input_text = [\"The little black cat sleeps in the window\", \"The dog runs in the field\"]\n",
    "prompt = \"Translate English into French: \"\n",
    "prompted_text = [prompt + in_text for in_text in input_text]\n",
    "encoded = tokenizer(prompted_text, return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "outputs = model.generate(**encoded, max_new_tokens=100)\n",
    "\n",
    "for out in outputs:\n",
    "    print(tokenizer.decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5577a4f-fbb6-43dc-a0cc-9164af8d2ab7",
   "metadata": {},
   "source": [
    "For this task, your goal is use the model to translate a large collection of German text into English, drawing from a paired translation of the novel Jane Eyre. Below we download and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f2655d1-f62a-4e32-a083-c6dc45085ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 101:\n",
      "German: Dieser Vorwurf meiner Abhängigkeit war in meinen Ohren fast zum leeren, bedeutungslosen Singsang geworden, sehr schmerzlich und bedrückend, aber nur halb verständlich.\n",
      "English: This reproach of my dependence had become a vague sing-song in my ear: very painful and crushing, but only half intelligible.\n",
      "\n",
      "Example 102:\n",
      "German: Nun fiel auch Miß Abbot ein: »Und Sie sollten auch nicht denken, daß Sie mit den Fräulein Reed und Mr. Reed auf gleicher Stufe stehen, weil Mrs. Reed Ihnen gütig erlaubt, mit ihren Kindern erzogen zu werden.\n",
      "English: Miss Abbot joined in-- \"And you ought not to think yourself on an equality with the Misses Reed and Master Reed, because Missis kindly allows you to be brought up with them.\n",
      "\n",
      "Example 103:\n",
      "German: Diese werden einmal ein großes Vermögen haben, und Sie sind arm. Sie müssen demütig und bescheiden sein und versuchen, sich den andern angenehm zu machen.«\n",
      "English: They will have a great deal of money, and you will have none: it is your place to be humble, and to try to make yourself agreeable to them.\"\n",
      "\n",
      "Total examples loaded: 500\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class OpusDataset(Dataset):\n",
    "    def __init__(self, dataset_stream, num_examples):\n",
    "        # Convert streaming dataset to list for random access\n",
    "        self.examples = list(dataset_stream.take(num_examples))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# Load the dataset in streaming mode\n",
    "dataset_stream = load_dataset(\n",
    "    \"opus_books\",\n",
    "    \"de-en\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "dataset = OpusDataset(dataset_stream, num_examples=500)\n",
    "\n",
    "for i in range(100, 103):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"German: {dataset[i]['translation']['de']}\")\n",
    "    print(f\"English: {dataset[i]['translation']['en']}\")\n",
    "print(f\"\\nTotal examples loaded: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded5cd5-b650-4057-a19a-3265eb25f0b3",
   "metadata": {},
   "source": [
    "**Use the Flan-T5 model to translate all of the German text in the dataset into English.** Even with GPU compute, this may take several minutes, but should not take hours. We encourage you to add some output every 10 or 50 examples so that you can track the progress, though you are not required to do so.\n",
    "\n",
    "**Select at least three examples from the dataset and print the model translation as well as the real English text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb5fc49-28eb-4ce5-8297-0999c70eed9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated 50/500 examples.\n",
      "Translated 100/500 examples.\n",
      "Translated 150/500 examples.\n",
      "Translated 200/500 examples.\n",
      "Translated 250/500 examples.\n",
      "Translated 300/500 examples.\n",
      "Translated 350/500 examples.\n",
      "Translated 400/500 examples.\n",
      "Translated 450/500 examples.\n",
      "Translated 500/500 examples.\n",
      "\n",
      "Example 101:\n",
      "German: Dieser Vorwurf meiner Abhängigkeit war in meinen Ohren fast zum leeren, bedeutungslosen Singsang geworden, sehr schmerzlich und bedrückend, aber nur halb verständlich.\n",
      "Real English: This reproach of my dependence had become a vague sing-song in my ear: very painful and crushing, but only half intelligible.\n",
      "Model Translation: This draft of my own was quickly becoming a painful and painful singing, very painful and painful, but only half a logical one.\n",
      "\n",
      "Example 201:\n",
      "German: Welche seltene Höflichkeit!\n",
      "Real English: Wonderful civility this!\n",
      "Model Translation: What rare happiness!\n",
      "\n",
      "Example 301:\n",
      "German: »Ich weiß es nicht.\n",
      "Real English: \"I don't know.\n",
      "Model Translation: \"I don't know.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "class OpusDataset(Dataset):\n",
    "    def __init__(self, dataset_stream, num_examples):\n",
    "        self.examples = list(dataset_stream.take(num_examples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "dataset_stream = load_dataset(\"opus_books\", \"de-en\", split=\"train\", streaming=True)\n",
    "dataset = OpusDataset(dataset_stream, num_examples=500)\n",
    "\n",
    "translations = []\n",
    "batch_size = 10  \n",
    "\n",
    "for i in range(0, len(dataset), batch_size):\n",
    "    batch = dataset[i:i + batch_size]\n",
    "    german_texts = [f\"Translate German to English: {example['translation']['de']}\" for example in batch]\n",
    "    \n",
    "    encoded = tokenizer(german_texts, return_tensors=\"pt\", padding=\"longest\", truncation=True).to(device)\n",
    "    \n",
    "    outputs = model.generate(**encoded, max_new_tokens=100)\n",
    "    batch_translations = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    \n",
    "    translations.extend(batch_translations)\n",
    "    \n",
    "    if (i + batch_size) % 50 == 0 or (i + batch_size) >= len(dataset):\n",
    "        print(f\"Translated {min(i + batch_size, len(dataset))}/{len(dataset)} examples.\")\n",
    "\n",
    "for i in [100, 200, 300]:\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"German: {dataset[i]['translation']['de']}\")\n",
    "    print(f\"Real English: {dataset[i]['translation']['en']}\")\n",
    "    print(f\"Model Translation: {translations[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e3308a-f0ab-429a-bbe4-71b80bac168e",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Translations are difficult to evaluate quantitatively and without expert human translators. One common metric is the [BLEU score](https://en.wikipedia.org/wiki/BLEU).\n",
    "\n",
    "The below example demonstrates calculating BLEU scores with the `evaluate` module from Hugging Face. You can [see the documentation here](https://huggingface.co/spaces/evaluate-metric/bleu). The first value in the results dictionary gives the score. Normally the score is reported on a scale from 0-100; this implementation reports it on a 0-1 scale. Higher values are better, but scores of 1 are not necessarily expected given the many possible ways to translate.\n",
    "\n",
    "Note that `predictions` is a list of strings, but `references` is a list of lists of strings. This is because a single predicted translation could potentially have multiple equally good reference translations. In our case however we just have the single translation per pair, so `references` will be a list of lists, each with a single element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31e3dbd0-2752-45bf-b1bf-9f77934aa31a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting evaluate\n",
      "  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/a2/e7/cbca9e2d2590eb9b5aa8f7ebabe1beb1498f9462d2ecede5c9fd9735faaf/evaluate-0.4.3-py3-none-any.whl.metadata\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./.local/lib/python3.11/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: dill in ./.local/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.11/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.local/lib/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.local/lib/python3.11/site-packages (from evaluate) (4.67.0)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./.local/lib/python3.11/site-packages (from evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./.local/lib/python3.11/site-packages (from evaluate) (0.26.2)\n",
      "Requirement already satisfied: packaging in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: filelock in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (18.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.11/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/pkg/miniconda-23.9.0/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "\u001b[33m  WARNING: The script evaluate-cli is installed in '/home/users/mll80/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8535dea9-e0f8-4b4d-831c-06fb4d32cca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.5555238068023582, 'precisions': [0.8, 0.6666666666666666, 0.5, 0.35714285714285715], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 20, 'reference_length': 20}\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "import evaluate\n",
    "\n",
    "predictions = [\"the black cat is sleeping in the sun by the window\", \n",
    "               \"the dog runs in the field while it rains\"]\n",
    "references = [[\"the black cat is sleeps on the sun by the window\"], \n",
    "              [\"the dog run in the field while it rain\"]]\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c882937-f116-45bd-bb4c-657fde8d4262",
   "metadata": {},
   "source": [
    "**Calculate the `BLEU` score of your translations from task 2 against the real English text`. Report your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9495ae5a-a30c-4849-b0e4-95334a440b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.15768461956270785\n"
     ]
    }
   ],
   "source": [
    "predictions = translations\n",
    "references = [[example['translation']['en']] for example in dataset]\n",
    "\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(f\"BLEU Score: {results['bleu']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c4927-f698-4bf0-a5f1-9863e99643cf",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4848987-6ee2-408d-a96d-670f0f546ba0",
   "metadata": {},
   "source": [
    "In this task, study the impact of model scale on the quality of the resulting translations. Earlier we used `model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.float16)` to import a `flan-t5-small` model. \n",
    "\n",
    "**Use two additional models: `flan-t5-base` and `flan-t5-large` to generate translations of the same dataset. Evaluate and report the BLEU score of both translations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "116da63c-ea78-4bb3-a2ef-d22c0089b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: google/flan-t5-small\n",
      "BLEU Score for google/flan-t5-small: 0.07267746503411557\n",
      "Processing model: google/flan-t5-base\n",
      "BLEU Score for google/flan-t5-base: 0.1226615702011814\n",
      "Processing model: google/flan-t5-large\n",
      "BLEU Score for google/flan-t5-large: 0.15768461956270785\n",
      "\n",
      "BLEU Scores for Different Models:\n",
      "google/flan-t5-small: 0.07267746503411557\n",
      "google/flan-t5-base: 0.1226615702011814\n",
      "google/flan-t5-large: 0.15768461956270785\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "model_names = [\"google/flan-t5-small\", \"google/flan-t5-base\", \"google/flan-t5-large\"]\n",
    "\n",
    "# Dictionary to store BLEU scores for each model\n",
    "bleu_scores = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    translations = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        german_text = example['translation']['de']\n",
    "        input_text = f\"Translate German to English: {german_text}\"\n",
    "        \n",
    "        encoded = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        output = model.generate(**encoded, max_new_tokens=100)\n",
    "        translated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        translations.append(translated_text)\n",
    "    \n",
    "    references = [[example['translation']['en']] for example in dataset]\n",
    "    \n",
    "    results = bleu.compute(predictions=translations, references=references)\n",
    "    bleu_scores[model_name] = results['bleu']\n",
    "    print(f\"BLEU Score for {model_name}: {results['bleu']}\")\n",
    "\n",
    "print(\"\\nBLEU Scores for Different Models:\")\n",
    "for model_name, score in bleu_scores.items():\n",
    "    print(f\"{model_name}: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
