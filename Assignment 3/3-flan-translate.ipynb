{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1001c433-4fca-4ba3-a670-10275b9274ad",
   "metadata": {},
   "source": [
    "# Part 3: Translation with Flan-T5\n",
    "\n",
    "In this part you will experiment with Google Flan-T5 models for machine translation. The original T5 model was a unifed sequence-to-sequence encoder-decoder architecture pretrained on a variety of tasks including machine translation. The Flan line of models improved on the performance of the original T5 series.\n",
    "\n",
    "In this part we will only apply the models, not train them. We will evaluate our results using the [Bleu score](https://en.wikipedia.org/wiki/BLEU), a common (but not perfect) quantitative metric for evaluating the quality of translations. Flan-T5 also comes in several different model sizes: We will study the impact of model size on performance by considering several different versions of Flan-T5.\n",
    "\n",
    "**Learning objectives.** You will:\n",
    "1. Examine an encoder-decoder sequence-to-sequence Flan-T5 transformer model\n",
    "2. Apply Flan-T5 models to perform machine translation\n",
    "3. Evaluate the quality of machine translations by computing Bleu scores with respect to reference translations\n",
    "4. Study the affect\n",
    "\n",
    "While it is possible to complete this assignment using CPU compute, it may be slow. To accelerate your training, consider using GPU resources such as `CUDA` through the CS department cluster. Alternatives include Google colab or local GPU resources for those running on machines with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5249219-8c67-4087-b2be-3f6c9982d951",
   "metadata": {},
   "source": [
    "First, ensure that you have the `transformers` and `datasets` modules installed. We will use these modules for importing tokenizers, pretrained models, and datasets. You can run the following cells to try to install them with `pip` if needed. If you are using ondemand, ideally you would simply include `module load transformers` and `module load datasets` when making your initial reservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bfb3fa3-39f4-45ca-989d-31019c339100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\micha\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\micha\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\micha\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\micha\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\micha\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f166e33-29ad-45b3-93f1-96b3f8173b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\micha\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\micha\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\micha\\anaconda3\\lib\\site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\micha\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9aba23-9b2c-4aec-beda-a00cefa676df",
   "metadata": {},
   "source": [
    "First we import the `flan-t5` tokenizer (shared across all model sizes) and demonstrate its characteristics and usage. Note that the API is the same as we saw previously for the `BERT` model -- may want to review the extra details in that earlier part.\n",
    "\n",
    "Note that the example contains both English and French text.\n",
    "\n",
    "(If you have trouble downloading the tokenizer, it is possible that you need to install the `sentencepiece` module, for example by `pip install sentencepiece`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9819fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\micha\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54dfc5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\micha\\anaconda3\\lib\\site-packages (8.0.4)\n",
      "Collecting ipywidgets\n",
      "  Obtaining dependency information for ipywidgets from https://files.pythonhosted.org/packages/22/2d/9c0b76f2f9cc0ebede1b9371b6f317243028ed60b90705863d493bae622e/ipywidgets-8.1.5-py3-none-any.whl.metadata\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting comm>=0.1.3 (from ipywidgets)\n",
      "  Obtaining dependency information for comm>=0.1.3 from https://files.pythonhosted.org/packages/e6/75/49e5bfe642f71f272236b5b2d2691cf915a7283cc0ceda56357b61daa538/comm-0.2.2-py3-none-any.whl.metadata\n",
      "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from ipywidgets) (5.7.1)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Obtaining dependency information for widgetsnbextension~=4.0.12 from https://files.pythonhosted.org/packages/21/02/88b65cc394961a60c43c70517066b6b679738caf78506a5da7b88ffcb643/widgetsnbextension-4.0.13-py3-none-any.whl.metadata\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Obtaining dependency information for jupyterlab-widgets~=3.0.12 from https://files.pythonhosted.org/packages/a9/93/858e87edc634d628e5d752ba944c2833133a28fa87bb093e6832ced36a3e/jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: backcall in c:\\users\\micha\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\micha\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\micha\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\micha\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\micha\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\micha\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\micha\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\micha\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\micha\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\micha\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\micha\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\micha\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.8 kB ? eta -:--:--\n",
      "   ----------- --------------------------- 41.0/139.8 kB 653.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 122.9/139.8 kB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- 139.8/139.8 kB 918.6 kB/s eta 0:00:00\n",
      "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "   ---------------------------------------- 0.0/214.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 214.4/214.4 kB 13.6 MB/s eta 0:00:00\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.3 MB 14.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.3 MB 14.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.2/2.3 MB 15.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 13.5 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, comm, ipywidgets\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 4.0.5\n",
      "    Uninstalling widgetsnbextension-4.0.5:\n",
      "      Successfully uninstalled widgetsnbextension-4.0.5\n",
      "  Attempting uninstall: jupyterlab-widgets\n",
      "    Found existing installation: jupyterlab-widgets 3.0.5\n",
      "    Uninstalling jupyterlab-widgets-3.0.5:\n",
      "      Successfully uninstalled jupyterlab-widgets-3.0.5\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.1.2\n",
      "    Uninstalling comm-0.1.2:\n",
      "      Successfully uninstalled comm-0.1.2\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 8.0.4\n",
      "    Uninstalling ipywidgets-8.0.4:\n",
      "      Successfully uninstalled ipywidgets-8.0.4\n",
      "Successfully installed comm-0.2.2 ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "    WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\scipy-1.12.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e637e38-7a1f-4eb3-91b4-e080daf23234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f3a5ceb8c54b20824d648624a0b9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\micha\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bffcaa01fc4719b69e60abc08a926f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a28544e5af34d69a23574abbbbb4a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  32100\n",
      "{'input_ids': [[37, 385, 1001, 1712, 2085, 7, 16, 8, 2034, 1], [312, 4561, 3582, 9691, 5048, 247, 50, 25301, 1, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "print(\"Vocabulary size: \", tokenizer.vocab_size)\n",
    "tokenized = tokenizer([\"The little black cat sleeps in the window\", \n",
    "                       \"Le petit chat noir dort dans la fenêtre\"], padding='longest')\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b009142-7ad6-423d-9447-cf362ac3e6ae",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Below we import and preview the `flan-t5` model, beginning with the small version. You will also note that we are using 16-bit float representations to save on memory (this may be particularly relevant if you are using GPU compute for larger models, where the GPU may have limited memory available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef167a43-0dba-41eb-bb2f-7966b1a9b34c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# run, but you do not need to modify this code\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.float32)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe34963-7122-4021-bb4b-63b1d210c480",
   "metadata": {},
   "source": [
    "Examine the `model.parameters()`. How much memory (in kilobytes (KB), megabytes (MB), or gigabytes (GB)) should it take to store the model itself, given the 16-bit (or 2 byte) precision specified in the import? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41035ae7-9718-4d2e-acab-7d6d26117c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76961152\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b89f06-b05c-4882-b0fd-708fd2ed34a3",
   "metadata": {},
   "source": [
    "Given the 2 byte precision specified in the import, the memory in total bytes is the number of parameters * bytes per parameter, which is 76,961,152 * 2, approximately 154,000,000 bytes, or about 146.5 MB (dividing 154,000,000 by 1024 * 1024). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9a63d-7d7f-481f-a7a8-e7d87143c297",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Now we import and demonstrate the basic usage of the model `generate` method. This method autoregressively generates new text as we have discussed before in the context of causal language modeling, and supports different approaches (greedy, beam search, and sampling). The `generate` method API is [documented here](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/text_generation#transformers.GenerationMixin.generate).\n",
    "\n",
    "The example below demonstrates encoding a *batch* of inputs and passing them to the model for autoregressive generation. The printed output is generated by the model for the first and second input in the batch respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57177033-b02f-4e4a-afb0-f380313e3731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,   37, 1712,   19,    3,    9, 1712,    5,    1,    0],\n",
      "        [   0,   37, 1782,   19, 1180,   16,    8, 1057,    5,    1]])\n",
      "The cat is a cat.\n",
      "The dog is running in the field.\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "input_text = [\"The little black cat sleeps in the window\", \"The dog runs in the field\"]\n",
    "encoded = tokenizer(input_text, return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "outputs = model.generate(**encoded, max_new_tokens=100)\n",
    "print(outputs)\n",
    "\n",
    "for out in outputs:\n",
    "    print(tokenizer.decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa24589-2848-4928-b8c4-5869b9e31112",
   "metadata": {},
   "source": [
    "Note that the model did not translate the inputs. That is by design: Flan-T5 was pretrained on several different tasks including but not limited to machine translation.\n",
    "\n",
    "In order to use the model for translation, we need to **prompt it** to do so, providing context (literally) connecting to its pretraining. There are several different promptings that should work, we demonstrate two below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f8c4dc-11cd-401f-bb8a-c08fe7baba8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La petite cat noir s'est en vertu de la fenêtre.\n",
      "Le chien s'est en l'égard de la field.\n"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "input_text = [\"The little black cat sleeps in the window\", \"The dog runs in the field\"]\n",
    "prompt = \"Translate English into French: \"\n",
    "prompted_text = [prompt + in_text for in_text in input_text]\n",
    "encoded = tokenizer(prompted_text, return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "outputs = model.generate(**encoded, max_new_tokens=100)\n",
    "\n",
    "for out in outputs:\n",
    "    print(tokenizer.decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5577a4f-fbb6-43dc-a0cc-9164af8d2ab7",
   "metadata": {},
   "source": [
    "For this task, your goal is use the model to translate a large collection of German text into English, drawing from a paired translation of the novel Jane Eyre. Below we download and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f2655d1-f62a-4e32-a083-c6dc45085ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 101:\n",
      "German: Der Sommerabend war lang und noch hell.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'en'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGerman: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal examples loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'en'"
     ]
    }
   ],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class OpusDataset(Dataset):\n",
    "    def __init__(self, dataset_stream, num_examples):\n",
    "        # Convert streaming dataset to list for random access\n",
    "        self.examples = list(dataset_stream.take(num_examples))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# Load the dataset in streaming mode\n",
    "dataset_stream = load_dataset(\n",
    "    \"opus_books\",\n",
    "    \"de-en\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Create instance of custom dataset\n",
    "dataset = OpusDataset(dataset_stream, num_examples=500)\n",
    "\n",
    "# Print a few examples to verify\n",
    "for i in range(100, 103):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"German: {dataset[i]['translation']['de']}\")\n",
    "    print(f\"English: {dataset[i]['translation']['en']}\")\n",
    "print(f\"\\nTotal examples loaded: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded5cd5-b650-4057-a19a-3265eb25f0b3",
   "metadata": {},
   "source": [
    "**Use the Flan-T5 model to translate all of the German text in the dataset into English.** Even with GPU compute, this may take several minutes, but should not take hours. We encourage you to add some output every 10 or 50 examples so that you can track the progress, though you are not required to do so.\n",
    "\n",
    "**Select at least three examples from the dataset and print the model translation as well as the real English text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5fc49-28eb-4ce5-8297-0999c70eed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for task 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e3308a-f0ab-429a-bbe4-71b80bac168e",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Translations are difficult to evaluate quantitatively and without expert human translators. One common metric is the [BLEU score](https://en.wikipedia.org/wiki/BLEU).\n",
    "\n",
    "The below example demonstrates calculating BLEU scores with the `evaluate` module from Hugging Face. You can [see the documentation here](https://huggingface.co/spaces/evaluate-metric/bleu). The first value in the results dictionary gives the score. Normally the score is reported on a scale from 0-100; this implementation reports it on a 0-1 scale. Higher values are better, but scores of 1 are not necessarily expected given the many possible ways to translate.\n",
    "\n",
    "Note that `predictions` is a list of strings, but `references` is a list of lists of strings. This is because a single predicted translation could potentially have multiple equally good reference translations. In our case however we just have the single translation per pair, so `references` will be a list of lists, each with a single element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535dea9-e0f8-4b4d-831c-06fb4d32cca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run but you do not need to modify this code\n",
    "\n",
    "import evaluate\n",
    "\n",
    "predictions = [\"the black cat is sleeping in the sun by the window\", \n",
    "               \"the dog runs in the field while it rains\"]\n",
    "references = [[\"the black cat is sleeps on the sun by the window\"], \n",
    "              [\"the dog run in the field while it rain\"]]\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c882937-f116-45bd-bb4c-657fde8d4262",
   "metadata": {},
   "source": [
    "**Calculate the `BLEU` score of your translations from task 2 against the real English text`. Report your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495ae5a-a30c-4849-b0e4-95334a440b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for task 3 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c4927-f698-4bf0-a5f1-9863e99643cf",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4848987-6ee2-408d-a96d-670f0f546ba0",
   "metadata": {},
   "source": [
    "In this task, study the impact of model scale on the quality of the resulting translations. Earlier we used `model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.float16)` to import a `flan-t5-small` model. \n",
    "\n",
    "**Use two additional models: `flan-t5-base` and `flan-t5-large` to generate translations of the same dataset. Evaluate and report the BLEU score of both translations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116da63c-ea78-4bb3-a2ef-d22c0089b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for task 4 here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
