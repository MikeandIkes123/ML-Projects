{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Deep Reinforcement Learning\n",
    "\n",
    "In this part of the assignment you will work on a more challenging control task in the [Lunar Lander environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). The state space is continuous and we will employ Deep Q-Learning (DQN) using a deep neural network to handle generalizing between states. \n",
    "\n",
    "**Learning objectives.** You will:\n",
    "1. Implement the Deep Q-learning (DQN) algorithm with epsilon greedy action selection\n",
    "2. Apply DQN to train an agent on a challenging control task with a large (continuous) state space\n",
    "3. Evaluate the learning curve of average episodic return over training\n",
    "\n",
    "While it is possible to complete this assignment using CPU compute, it may be slow. To accelerate your training, consider using GPU resources such as `CUDA` through the CS department cluster. Alternatives include Google colab or local GPU resources for those running on machines with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were able to run `pip install \"gymnasium[all]\"` in Part 1 then you may not need to install any additional dependencies to work with the Lunar Lander environment. However, if you were only able to install the base `gymnasium` then you may need to complete the additional installations of `swig` and `box2d` below. The first two commands use `pip` for the package manager, the third uses `conda` if you are using the Anaconda package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge swig box2d-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that the installation was successful by running the following code. The code attempts to import the gymnasium module, create a [Lunar Lander environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/), and then execute random actions in the environment. \n",
    "\n",
    "The environment is set to `render_mode=\"human\"`, meaning you should see a visualization of the environment with the agent taking random actions. Be aware that the visualization can be unstable on some platforms -- in particular, if your Python Kernel crashes after closing the visualization, that will not prevent you from completing the assignment (but is a good indication that you should save your learned Q tables/networks before rendering a visualization so that you don't lose any work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Initialise the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset()\n",
    "for _ in range(200):\n",
    "    # this is where you would insert your policy\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "As we will be implementing DQN, your first task is to define a deep neural network architecture to use to estimate the Q function during training. You should familiarize yourself with the [Lunar Lander environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/) documentation to understand the state/observation space, action space, and reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to approximate the Q-function in DQN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Define the network layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Input to first hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)  # First hidden to second hidden\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)  # Second hidden to output\n",
    "\n",
    "        # Initialization (optional but good practice for stable learning)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        :param x: Input state (observation)\n",
    "        :return: Q-values for all actions\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.fc1(x))  # First layer with ReLU activation\n",
    "        x = torch.relu(self.fc2(x))  # Second layer with ReLU activation\n",
    "        x = self.fc3(x)  # Output layer (raw Q-values)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Your next task is to define the `LunarLanderAgent` which should also implement the deep Q-learning (DQN) algorithm. The structure of the class is left up to you: While you are welcome to adopt a similar structure of `__init__`, `action_select`, and `update` as in Part 1, you are also free to change the structure as you see fit. \n",
    "\n",
    "The implementation will be more challenging than in the Part 1 tabular case. You may wish to review 11/25's lecture in particular for a discussion of the implementation. A few tips and reminders:\n",
    "1. You should use a main Q-network, the weights of which you will regular update, and a frozen target Q-network used to predict the future predictive target, the weights of which you will periodically copy from the main Q-network. You should copy these weights only everal several thousand steps in the environment for stability. Both should share the same architecture you defined above and should be saved as instance variables of your agent, along with a PyTorch loss MSE loss function and optimizer.\n",
    "2. You should use an experience replay buffer and employ batched updates to the weights of your Q-network. The easiest data structure to use for the experience replay buffer is probably the [Python collections deque](https://docs.python.org/3/library/collections.html#collections.deque). You probably want an experience replay buffer that is at least as large as 100,000 experiences, if not more.\n",
    "3. You will likely need a lower learning rate such as `0.001` or `0.0005`. Also, you will want a good amount of exploration early on. If using `epsilon` decay, you should decay only very slowly, as you should expect to need thousands of episodes to learn a good policy.\n",
    "4. While you are not required to use GPU training for Q-network operations, it will substantially accelerate training, and you are encouraged to do so. Make sure not to store all of the experiences in your entire experience replay buffer on the GPU. Instead, store those on CPU memory and only move batches on GPU before training. Otherwise you are likely to run out of memory during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class LunarLanderAgent:\n",
    "    \"\"\"\n",
    "    Implements a deep Q-learning agent for the Lunar Lander environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, device='cpu', replay_buffer_size=100000, batch_size=64, gamma=0.99, \n",
    "                 learning_rate=0.0005, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995, target_update_freq=1000):\n",
    "        self.device = device\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Initialize Q-networks\n",
    "        self.q_network = NeuralNet(state_dim, action_dim).to(self.device)\n",
    "        self.target_q_network = NeuralNet(state_dim, action_dim).to(self.device)\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())  # Copy weights\n",
    "        self.target_q_network.eval()  # Target network is not trained\n",
    "        \n",
    "        # Optimizer and loss\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        self.steps = 0\n",
    "\n",
    "    def action_select(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)  # Random action\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        return torch.argmax(q_values, dim=1).item()  # Exploit best action\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store a transition in the replay buffer.\n",
    "        \"\"\"\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Perform a single training step on the Q-network.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return  # Not enough samples to train\n",
    "\n",
    "        # Sample batch from replay buffer\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        # Compute Q-values for current states\n",
    "        q_values = self.q_network(states).gather(1, actions).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_q_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        \n",
    "        # Gradient descent\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # Periodically update target network\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target_q_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Train your DQN agent on the Lunar Lander environment. You should be able to learn a policy that achieves an average/expected episode return (cumulative reward) of approximately 200 or more -- **we will consider the training a success if the average/expected episode return converges to greater than 150**. To accomplish this you may need to experiment with several of the key hyperparameters for the learning agent. Note that the learning will likely be much more challenging and variable than in the Part 1 warmup task.\n",
    "\n",
    "While you are not required, you are encouraged to print periodic evaluation information during training (for example, every 100 or so episodes you may wish to printout the average return of the last 100 episodes) in order to track progress.\n",
    "\n",
    "Training will take quite some time on this environment: You should expect to need between **1 and 10 thousand episodes** of experience (You should use a small number between 100 and 1 thousand while prototyping your implementation, then several thousand for your final training). Even with GPU acceleration you should expect this to take **several minutes or as much as an hour** (for your final run over several thousand episodes). Partially for this reason, you are strongly encourage to **save your Q-network model parameters** after a substantial and long training run, to ensure you do not lose them. See the [PyTorch documentation on loading and saving model parameters](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
    "\n",
    "It is relatively common/likely on this lunar lander environment to get stuck in a local optimum where the agent learns to \"hover\" but not land. Landing correctly is difficult and crashing incurs a large negative reward, so sometimes the agent gets \"stuck\" in a policy of hovering in place without attempt to land. This results in an average return of around 0. If your agent is getting stuck like this, one recommendation is to modify the reward function by adding a per-timestep reward penalty during learning. You can do this very simply in the training loop by just reducing the reward as in `r = r - timestep_penalty` before passing the experience to your agent. A value for `timestep_penalty` between 0.1 and 0.5 may help to incentivize the agent not to \"hover\" but to actually attempt and learn how to land. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Assume LunarLanderAgent is already implemented and imported\n",
    "\n",
    "def train_dqn_agent_with_recording():\n",
    "    # Create the environment with wrapper for episode statistics\n",
    "    n_episodes = 100  # Adjust as needed for prototyping or final training\n",
    "    env = gym.make(\"LunarLander-v3\")\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Initialize DQN agent\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    agent = LunarLanderAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        device=device,\n",
    "        replay_buffer_size=100000,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        learning_rate=0.0005,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.1,\n",
    "        epsilon_decay=0.995,\n",
    "        target_update_freq=1000\n",
    "    )\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_episodes = 5000\n",
    "    max_timesteps = 500\n",
    "    timestep_penalty = 0.1\n",
    "    evaluation_interval = 100\n",
    "    model_save_path = \"dqn_lunar_lander.pth\"\n",
    "\n",
    "    # Logging for evaluation\n",
    "    rewards_history = []\n",
    "    evaluation_rewards = []\n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        for t in range(max_timesteps):\n",
    "            action = agent.action_select(state)  # Epsilon-greedy action selection\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Apply timestep penalty\n",
    "            reward -= timestep_penalty\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store transition in replay buffer and train\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            agent.update()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        # Track total rewards\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "        # Periodic evaluation\n",
    "        if episode % evaluation_interval == 0:\n",
    "            avg_return = np.mean(rewards_history[-evaluation_interval:])\n",
    "            evaluation_rewards.append(avg_return)\n",
    "            print(f\"Episode {episode}/{num_episodes}, Average Return: {avg_return:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "            # Save model\n",
    "            torch.save(agent.q_network.state_dict(), model_save_path)\n",
    "\n",
    "    # Close environment and save final model\n",
    "    env.close()\n",
    "    print(\"Training completed. Model saved at:\", model_save_path)\n",
    "    return evaluation_rewards\n",
    "\n",
    "# Run training\n",
    "if __name__ == \"__main__\":\n",
    "    evaluation_rewards = train_dqn_agent_with_recording()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "After training, you can use the `visualize_learning_curves` function below to create two plots: One of the moving average episode return over the course of training and another of the moving average episode length over the course of training. A `smoothing` parameter to the function determines how many episodes to average over. That is, the default of `smoothing=10` means that each point in the resulting figure is the average of ten episodes. This has the visual effect of *smoothing* out the appearance of the plot, which can make the overall trend easier to visualize even if individual episodes vary significantly.\n",
    "\n",
    "Create visualizations of your learning curves. You should see improving performance on average episode return converging toward approximately 150 or above. The learning may be more variable than in Part 1, so you may wish to increase the `smoothing` parameter to `50` or even `100` to make it easier to visualize the average trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run, but you do not need to modify this code\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def visualize_learning_curves(env, smoothing=10):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    axs[0].plot(np.convolve(env.return_queue, np.ones(smoothing), 'valid') / smoothing)\n",
    "    axs[0].set_title(\"Episode Returns\", fontsize=20)\n",
    "    axs[0].set_xlabel(\"Episode\", fontsize=20)\n",
    "    axs[0].set_ylabel(\"Return\", fontsize=20)\n",
    "\n",
    "    axs[1].plot(np.convolve(env.length_queue, np.ones(smoothing), 'valid') / smoothing)\n",
    "    axs[1].set_title(\"Episode Lengths\", fontsize=20)\n",
    "    axs[1].set_xlabel(\"Episode\", fontsize=20)\n",
    "    axs[1].set_ylabel(\"Length\", fontsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: visualize learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional\n",
    "\n",
    "The above evaluation of the learning curve was purely quantiative. While you are not required, you are invited and encouraged to qualitatively evaluate your learned policy/agent visually below. To do so, render the environment in `\"human\"` mode as shown below. This should render the visualization of the environment. Write a standard environment loop over several episodes (but far fewer than used for training, perhaps just 5 or so), and use the trained agent for action selection. For this evaluation purpose, you are welcome to set `epsilon` to a lower value or even 0, and you do not need to perform learning updates.\n",
    "\n",
    "Again, be aware that the visualization can be unstable on some platforms -- in particular, if your Python Kernel crashes after closing the visualization, that will not prevent you from completing the assignment (but is a good indication that you should save your learned Q tables/networks before rendering a visualization so that you don't lose any work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "# optional todo: environment loop to visualize learned policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
